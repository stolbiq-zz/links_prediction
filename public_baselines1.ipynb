{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "\n",
    "###################\n",
    "# random baseline #\n",
    "###################\n",
    "\n",
    "random_predictions = np.random.choice([0, 1], size=len(testing_set))\n",
    "random_predictions = zip(range(len(testing_set)),random_predictions)\n",
    "\n",
    "with open(\"random_predictions.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    for row in random_predictions:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "# note: Kaggle requires that you add \"ID\" and \"category\" column headers\n",
    "\n",
    "###############################\n",
    "# beating the random baseline #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the following script gets an F1 score of approximately 0.66\n",
    "\n",
    "# data loading and preprocessing \n",
    "\n",
    "# the columns of the data frame below are: \n",
    "# (1) paper unique ID (integer)\n",
    "# (2) publication year (integer)\n",
    "# (3) paper title (string)\n",
    "# (4) authors (strings separated by ,)\n",
    "# (5) name of journal (optional) (string)\n",
    "# (6) abstract (string) - lowercased, free of punctuation except intra-word dashes\n",
    "\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstracts = [element[5] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(string, stemming=True, stopwords=True):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    if stopwords:\n",
    "        tokens = [token for token in tokens if token not in stpwds]\n",
    "    if stemming:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import igraph\n",
    "import string\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "def clean_text_simple(text, remove_stopwords=True, pos_filtering=True, stemming=True):\n",
    "    \n",
    "    punct = string.punctuation.replace('-', '')\n",
    "    \n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove punctuation (preserving intra-word dashes)\n",
    "    text = ''.join(l for l in text if l not in punct)\n",
    "    # strip extra white space\n",
    "    text = re.sub(' +',' ',text)\n",
    "    # strip leading and trailing white space\n",
    "    text = text.strip()\n",
    "    # tokenize (split based on whitespace)\n",
    "    ### fill the gap ###\n",
    "    tokens = text.split(' ')\n",
    "    if pos_filtering == True:\n",
    "        # apply POS-tagging\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        # retain only nouns and adjectives\n",
    "        tokens_keep = []\n",
    "        for i in range(len(tagged_tokens)):\n",
    "            item = tagged_tokens[i]\n",
    "            if (\n",
    "            item[1] == 'NN' or\n",
    "            item[1] == 'NNS' or\n",
    "            item[1] == 'NNP' or\n",
    "            item[1] == 'NNPS' or\n",
    "            item[1] == 'JJ' or\n",
    "            item[1] == 'JJS' or\n",
    "            item[1] == 'JJR'\n",
    "            ):\n",
    "                tokens_keep.append(item[0])\n",
    "        tokens = tokens_keep\n",
    "    if remove_stopwords:\n",
    "        stpwds = stopwords.words('english')\n",
    "        # remove stopwords\n",
    "        token_keep = [word for word in tokens if not word in stpwds]\n",
    "        tokens = token_keep\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        # apply Porter's stemmer\n",
    "        tokens_stemmed = list()\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                tokens_stemmed.append(stemmer.stem(token))\n",
    "            except IndexError:\n",
    "                tokens_stemmed.append(token)\n",
    "        tokens = tokens_stemmed\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "\t\n",
    "def terms_to_graph(terms, w):\n",
    "    # This function returns a directed, weighted igraph from a list of terms (the tokens from the pre-processed text) e.g., ['quick','brown','fox']\n",
    "    # Edges are weighted based on term co-occurence within a sliding window of fixed size 'w'\n",
    "    \n",
    "    from_to = {}\n",
    "    \n",
    "    # create initial graph (first w terms)\n",
    "    terms_temp = terms[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "    \n",
    "    new_edges = []\n",
    "    \n",
    "    for i in range(len(indexes)):\n",
    "        new_edges.append(' '.join(list(terms_temp[i] for i in indexes[i])))\n",
    "       \n",
    "    for i in range(0,len(new_edges)):\n",
    "        from_to[new_edges[i].split()[0],new_edges[i].split()[1]] = 1\n",
    "\n",
    "    # then iterate over the remaining terms\n",
    "    for i in range(w, len(terms)):\n",
    "        # term to consider\n",
    "        considered_term = terms[i]\n",
    "        # all terms within sliding window\n",
    "        terms_temp = terms[(i-w+1):(i+1)]\n",
    "\n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in range(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "    \n",
    "        for try_edge in candidate_edges:\n",
    "            \n",
    "            # if not self-edge\n",
    "            if try_edge[1] != try_edge[0]:\n",
    "                if try_edge in from_to:\n",
    "                # if edge has already been seen, update its weight\n",
    "                    from_to[try_edge] += 1\n",
    "                # if edge has never been seen, create it and assign it a unit weight     \n",
    "                else:\n",
    "                    from_to[try_edge] = 1\n",
    "    \n",
    "    # create empty graph\n",
    "    g = igraph.Graph(directed=True)\n",
    "    \n",
    "    # add vertices\n",
    "    g.add_vertices(sorted(set(terms)))\n",
    "    \n",
    "    # add edges, direction is preserved since the graph is directed\n",
    "    g.add_edges(from_to.keys())\n",
    "    \n",
    "    # set edge and vertice weights\n",
    "    g.es['weight'] = from_to.values() # based on co-occurence within sliding window\n",
    "    g.vs['weight'] = g.strength(weights=np.array([from_to.values()])) # weighted degree\n",
    "    \n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 abstracts processed\n",
      "2000 abstracts processed\n",
      "3000 abstracts processed\n",
      "4000 abstracts processed\n",
      "5000 abstracts processed\n",
      "6000 abstracts processed\n",
      "7000 abstracts processed\n",
      "8000 abstracts processed\n",
      "9000 abstracts processed\n",
      "10000 abstracts processed\n",
      "11000 abstracts processed\n",
      "12000 abstracts processed\n",
      "13000 abstracts processed\n",
      "14000 abstracts processed\n",
      "15000 abstracts processed\n",
      "16000 abstracts processed\n",
      "17000 abstracts processed\n",
      "18000 abstracts processed\n",
      "19000 abstracts processed\n",
      "20000 abstracts processed\n",
      "21000 abstracts processed\n",
      "22000 abstracts processed\n",
      "23000 abstracts processed\n",
      "24000 abstracts processed\n",
      "25000 abstracts processed\n",
      "26000 abstracts processed\n",
      "27000 abstracts processed\n"
     ]
    }
   ],
   "source": [
    "abstracts_cleaned = []\n",
    "counter = 0\n",
    "\n",
    "for abstract in abstracts:\n",
    "    #my_tokens = clean_text_simple(abstract)\n",
    "    my_tokens = preprocess(abstract)\n",
    "    my_tokens = nltk.Text(my_tokens)\n",
    "    abstracts_cleaned.append(my_tokens)\n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        print (counter, 'abstracts processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "iterable must yield numbers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-be78e8dfe61b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;31m# create graph-of-words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterms_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"th abstract not in graph: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e7f0fce23148>\u001b[0m in \u001b[0;36mterms_to_graph\u001b[0;34m(terms, w)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[1;31m# set edge and vertice weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_to\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# based on co-occurence within sliding window\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrom_to\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# weighted degree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: iterable must yield numbers"
     ]
    }
   ],
   "source": [
    "keywords_gow = []\n",
    "counter = 0\n",
    "\n",
    "for abstract in abstracts_cleaned:\n",
    "    # create graph-of-words\n",
    "    try:\n",
    "        g = terms_to_graph(abstract, w=4)\n",
    "    except IndexError:\n",
    "        print(str(counter) + \"th abstract not in graph: \" + str(abstract))\n",
    "        continue\n",
    "    # decompose graph-of-words\n",
    "    core_numbers = dict(zip(g.vs['name'],g.coreness()))\n",
    "    # retain main core as keywords\n",
    "    max_c_n = max(core_numbers.values())\n",
    "    keywords = [kwd for kwd, c_n in core_numbers.iteritems() if c_n == max_c_n]\n",
    "    # save results\n",
    "    keywords_gow.append(keywords)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        print (counter, 'abstracts processed')\n",
    "\n",
    "gen_keywords_gow = set(list(itertools.chain.from_iterable(keywords_gow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irina\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "def word_averaging(model, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in model.vocab:\n",
    "            vec = model.syn0[model.vocab[word].index]\n",
    "            vec_norm = vec/np.linalg.norm(vec)\n",
    "            mean.append(vec_norm)\n",
    "            all_words.add(model.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(model.layer_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(model, text_list):\n",
    "    return np.vstack([word_averaging(model, preprocess(review)) for review in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_mean = word_averaging_list(model, abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1_vec2_similarity =  1 - scipy.spatial.distance.cosine(vec1, vec2)\n",
    "    return vec1_vec2_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n",
      "77001 training examples processsed\n",
      "78001 training examples processsed\n",
      "79001 training examples processsed\n",
      "80001 training examples processsed\n",
      "81001 training examples processsed\n",
      "82001 training examples processsed\n",
      "83001 training examples processsed\n",
      "84001 training examples processsed\n",
      "85001 training examples processsed\n",
      "86001 training examples processsed\n",
      "87001 training examples processsed\n",
      "88001 training examples processsed\n",
      "89001 training examples processsed\n",
      "90001 training examples processsed\n",
      "91001 training examples processsed\n",
      "92001 training examples processsed\n",
      "93001 training examples processsed\n",
      "94001 training examples processsed\n",
      "95001 training examples processsed\n",
      "96001 training examples processsed\n",
      "97001 training examples processsed\n",
      "98001 training examples processsed\n",
      "99001 training examples processsed\n",
      "100001 training examples processsed\n",
      "101001 training examples processsed\n",
      "102001 training examples processsed\n",
      "103001 training examples processsed\n",
      "104001 training examples processsed\n",
      "105001 training examples processsed\n",
      "106001 training examples processsed\n",
      "107001 training examples processsed\n",
      "108001 training examples processsed\n",
      "109001 training examples processsed\n",
      "110001 training examples processsed\n",
      "111001 training examples processsed\n",
      "112001 training examples processsed\n",
      "113001 training examples processsed\n",
      "114001 training examples processsed\n",
      "115001 training examples processsed\n",
      "116001 training examples processsed\n",
      "117001 training examples processsed\n",
      "118001 training examples processsed\n",
      "119001 training examples processsed\n",
      "120001 training examples processsed\n",
      "121001 training examples processsed\n",
      "122001 training examples processsed\n",
      "123001 training examples processsed\n",
      "124001 training examples processsed\n",
      "125001 training examples processsed\n",
      "126001 training examples processsed\n",
      "127001 training examples processsed\n",
      "128001 training examples processsed\n",
      "129001 training examples processsed\n",
      "130001 training examples processsed\n",
      "131001 training examples processsed\n",
      "132001 training examples processsed\n",
      "133001 training examples processsed\n",
      "134001 training examples processsed\n",
      "135001 training examples processsed\n",
      "136001 training examples processsed\n",
      "137001 training examples processsed\n",
      "138001 training examples processsed\n",
      "139001 training examples processsed\n",
      "140001 training examples processsed\n",
      "141001 training examples processsed\n",
      "142001 training examples processsed\n",
      "143001 training examples processsed\n",
      "144001 training examples processsed\n",
      "145001 training examples processsed\n",
      "146001 training examples processsed\n",
      "147001 training examples processsed\n",
      "148001 training examples processsed\n",
      "149001 training examples processsed\n",
      "150001 training examples processsed\n",
      "151001 training examples processsed\n",
      "152001 training examples processsed\n",
      "153001 training examples processsed\n",
      "154001 training examples processsed\n",
      "155001 training examples processsed\n",
      "156001 training examples processsed\n",
      "157001 training examples processsed\n",
      "158001 training examples processsed\n",
      "159001 training examples processsed\n",
      "160001 training examples processsed\n",
      "161001 training examples processsed\n",
      "162001 training examples processsed\n",
      "163001 training examples processsed\n",
      "164001 training examples processsed\n",
      "165001 training examples processsed\n",
      "166001 training examples processsed\n",
      "167001 training examples processsed\n",
      "168001 training examples processsed\n",
      "169001 training examples processsed\n",
      "170001 training examples processsed\n",
      "171001 training examples processsed\n",
      "172001 training examples processsed\n",
      "173001 training examples processsed\n",
      "174001 training examples processsed\n",
      "175001 training examples processsed\n",
      "176001 training examples processsed\n",
      "177001 training examples processsed\n",
      "178001 training examples processsed\n",
      "179001 training examples processsed\n",
      "180001 training examples processsed\n",
      "181001 training examples processsed\n",
      "182001 training examples processsed\n",
      "183001 training examples processsed\n",
      "184001 training examples processsed\n",
      "185001 training examples processsed\n",
      "186001 training examples processsed\n",
      "187001 training examples processsed\n",
      "188001 training examples processsed\n",
      "189001 training examples processsed\n",
      "190001 training examples processsed\n",
      "191001 training examples processsed\n",
      "192001 training examples processsed\n",
      "193001 training examples processsed\n",
      "194001 training examples processsed\n",
      "195001 training examples processsed\n",
      "196001 training examples processsed\n",
      "197001 training examples processsed\n",
      "198001 training examples processsed\n",
      "199001 training examples processsed\n",
      "200001 training examples processsed\n",
      "201001 training examples processsed\n",
      "202001 training examples processsed\n",
      "203001 training examples processsed\n",
      "204001 training examples processsed\n",
      "205001 training examples processsed\n",
      "206001 training examples processsed\n",
      "207001 training examples processsed\n",
      "208001 training examples processsed\n",
      "209001 training examples processsed\n",
      "210001 training examples processsed\n",
      "211001 training examples processsed\n",
      "212001 training examples processsed\n",
      "213001 training examples processsed\n",
      "214001 training examples processsed\n",
      "215001 training examples processsed\n",
      "216001 training examples processsed\n",
      "217001 training examples processsed\n",
      "218001 training examples processsed\n",
      "219001 training examples processsed\n",
      "220001 training examples processsed\n",
      "221001 training examples processsed\n",
      "222001 training examples processsed\n",
      "223001 training examples processsed\n",
      "224001 training examples processsed\n",
      "225001 training examples processsed\n",
      "226001 training examples processsed\n",
      "227001 training examples processsed\n",
      "228001 training examples processsed\n",
      "229001 training examples processsed\n",
      "230001 training examples processsed\n",
      "231001 training examples processsed\n",
      "232001 training examples processsed\n",
      "233001 training examples processsed\n",
      "234001 training examples processsed\n",
      "235001 training examples processsed\n",
      "236001 training examples processsed\n",
      "237001 training examples processsed\n",
      "238001 training examples processsed\n",
      "239001 training examples processsed\n",
      "240001 training examples processsed\n",
      "241001 training examples processsed\n",
      "242001 training examples processsed\n",
      "243001 training examples processsed\n",
      "244001 training examples processsed\n",
      "245001 training examples processsed\n",
      "246001 training examples processsed\n",
      "247001 training examples processsed\n",
      "248001 training examples processsed\n",
      "249001 training examples processsed\n",
      "250001 training examples processsed\n",
      "251001 training examples processsed\n",
      "252001 training examples processsed\n",
      "253001 training examples processsed\n",
      "254001 training examples processsed\n",
      "255001 training examples processsed\n",
      "256001 training examples processsed\n",
      "257001 training examples processsed\n",
      "258001 training examples processsed\n",
      "259001 training examples processsed\n",
      "260001 training examples processsed\n",
      "261001 training examples processsed\n",
      "262001 training examples processsed\n",
      "263001 training examples processsed\n",
      "264001 training examples processsed\n",
      "265001 training examples processsed\n",
      "266001 training examples processsed\n",
      "267001 training examples processsed\n",
      "268001 training examples processsed\n",
      "269001 training examples processsed\n",
      "270001 training examples processsed\n",
      "271001 training examples processsed\n",
      "272001 training examples processsed\n",
      "273001 training examples processsed\n",
      "274001 training examples processsed\n",
      "275001 training examples processsed\n",
      "276001 training examples processsed\n",
      "277001 training examples processsed\n",
      "278001 training examples processsed\n",
      "279001 training examples processsed\n",
      "280001 training examples processsed\n",
      "281001 training examples processsed\n",
      "282001 training examples processsed\n",
      "283001 training examples processsed\n",
      "284001 training examples processsed\n",
      "285001 training examples processsed\n",
      "286001 training examples processsed\n",
      "287001 training examples processsed\n",
      "288001 training examples processsed\n",
      "289001 training examples processsed\n",
      "290001 training examples processsed\n",
      "291001 training examples processsed\n",
      "292001 training examples processsed\n",
      "293001 training examples processsed\n",
      "294001 training examples processsed\n",
      "295001 training examples processsed\n",
      "296001 training examples processsed\n",
      "297001 training examples processsed\n",
      "298001 training examples processsed\n",
      "299001 training examples processsed\n",
      "300001 training examples processsed\n",
      "301001 training examples processsed\n",
      "302001 training examples processsed\n",
      "303001 training examples processsed\n",
      "304001 training examples processsed\n",
      "305001 training examples processsed\n",
      "306001 training examples processsed\n",
      "307001 training examples processsed\n",
      "308001 training examples processsed\n",
      "309001 training examples processsed\n",
      "310001 training examples processsed\n",
      "311001 training examples processsed\n",
      "312001 training examples processsed\n",
      "313001 training examples processsed\n",
      "314001 training examples processsed\n",
      "315001 training examples processsed\n",
      "316001 training examples processsed\n",
      "317001 training examples processsed\n",
      "318001 training examples processsed\n",
      "319001 training examples processsed\n",
      "320001 training examples processsed\n",
      "321001 training examples processsed\n",
      "322001 training examples processsed\n",
      "323001 training examples processsed\n",
      "324001 training examples processsed\n",
      "325001 training examples processsed\n",
      "326001 training examples processsed\n",
      "327001 training examples processsed\n",
      "328001 training examples processsed\n",
      "329001 training examples processsed\n",
      "330001 training examples processsed\n",
      "331001 training examples processsed\n",
      "332001 training examples processsed\n",
      "333001 training examples processsed\n",
      "334001 training examples processsed\n",
      "335001 training examples processsed\n",
      "336001 training examples processsed\n",
      "337001 training examples processsed\n",
      "338001 training examples processsed\n",
      "339001 training examples processsed\n",
      "340001 training examples processsed\n",
      "341001 training examples processsed\n",
      "342001 training examples processsed\n",
      "343001 training examples processsed\n",
      "344001 training examples processsed\n",
      "345001 training examples processsed\n",
      "346001 training examples processsed\n",
      "347001 training examples processsed\n",
      "348001 training examples processsed\n",
      "349001 training examples processsed\n",
      "350001 training examples processsed\n",
      "351001 training examples processsed\n",
      "352001 training examples processsed\n",
      "353001 training examples processsed\n",
      "354001 training examples processsed\n",
      "355001 training examples processsed\n",
      "356001 training examples processsed\n",
      "357001 training examples processsed\n",
      "358001 training examples processsed\n",
      "359001 training examples processsed\n",
      "360001 training examples processsed\n",
      "361001 training examples processsed\n",
      "362001 training examples processsed\n",
      "363001 training examples processsed\n",
      "364001 training examples processsed\n",
      "365001 training examples processsed\n",
      "366001 training examples processsed\n",
      "367001 training examples processsed\n",
      "368001 training examples processsed\n",
      "369001 training examples processsed\n",
      "370001 training examples processsed\n",
      "371001 training examples processsed\n",
      "372001 training examples processsed\n",
      "373001 training examples processsed\n",
      "374001 training examples processsed\n",
      "375001 training examples processsed\n",
      "376001 training examples processsed\n",
      "377001 training examples processsed\n",
      "378001 training examples processsed\n",
      "379001 training examples processsed\n",
      "380001 training examples processsed\n",
      "381001 training examples processsed\n",
      "382001 training examples processsed\n",
      "383001 training examples processsed\n",
      "384001 training examples processsed\n",
      "385001 training examples processsed\n",
      "386001 training examples processsed\n",
      "387001 training examples processsed\n",
      "388001 training examples processsed\n",
      "389001 training examples processsed\n",
      "390001 training examples processsed\n",
      "391001 training examples processsed\n",
      "392001 training examples processsed\n",
      "393001 training examples processsed\n",
      "394001 training examples processsed\n",
      "395001 training examples processsed\n",
      "396001 training examples processsed\n",
      "397001 training examples processsed\n",
      "398001 training examples processsed\n",
      "399001 training examples processsed\n",
      "400001 training examples processsed\n",
      "401001 training examples processsed\n",
      "402001 training examples processsed\n",
      "403001 training examples processsed\n",
      "404001 training examples processsed\n",
      "405001 training examples processsed\n",
      "406001 training examples processsed\n",
      "407001 training examples processsed\n",
      "408001 training examples processsed\n",
      "409001 training examples processsed\n",
      "410001 training examples processsed\n",
      "411001 training examples processsed\n",
      "412001 training examples processsed\n",
      "413001 training examples processsed\n",
      "414001 training examples processsed\n",
      "415001 training examples processsed\n",
      "416001 training examples processsed\n",
      "417001 training examples processsed\n",
      "418001 training examples processsed\n",
      "419001 training examples processsed\n",
      "420001 training examples processsed\n",
      "421001 training examples processsed\n",
      "422001 training examples processsed\n",
      "423001 training examples processsed\n",
      "424001 training examples processsed\n",
      "425001 training examples processsed\n",
      "426001 training examples processsed\n",
      "427001 training examples processsed\n",
      "428001 training examples processsed\n",
      "429001 training examples processsed\n",
      "430001 training examples processsed\n",
      "431001 training examples processsed\n",
      "432001 training examples processsed\n",
      "433001 training examples processsed\n",
      "434001 training examples processsed\n",
      "435001 training examples processsed\n",
      "436001 training examples processsed\n",
      "437001 training examples processsed\n",
      "438001 training examples processsed\n",
      "439001 training examples processsed\n",
      "440001 training examples processsed\n",
      "441001 training examples processsed\n",
      "442001 training examples processsed\n",
      "443001 training examples processsed\n",
      "444001 training examples processsed\n",
      "445001 training examples processsed\n",
      "446001 training examples processsed\n",
      "447001 training examples processsed\n",
      "448001 training examples processsed\n",
      "449001 training examples processsed\n",
      "450001 training examples processsed\n",
      "451001 training examples processsed\n",
      "452001 training examples processsed\n",
      "453001 training examples processsed\n",
      "454001 training examples processsed\n",
      "455001 training examples processsed\n",
      "456001 training examples processsed\n",
      "457001 training examples processsed\n",
      "458001 training examples processsed\n",
      "459001 training examples processsed\n",
      "460001 training examples processsed\n",
      "461001 training examples processsed\n",
      "462001 training examples processsed\n",
      "463001 training examples processsed\n",
      "464001 training examples processsed\n",
      "465001 training examples processsed\n",
      "466001 training examples processsed\n",
      "467001 training examples processsed\n",
      "468001 training examples processsed\n",
      "469001 training examples processsed\n",
      "470001 training examples processsed\n",
      "471001 training examples processsed\n",
      "472001 training examples processsed\n",
      "473001 training examples processsed\n",
      "474001 training examples processsed\n",
      "475001 training examples processsed\n",
      "476001 training examples processsed\n",
      "477001 training examples processsed\n",
      "478001 training examples processsed\n",
      "479001 training examples processsed\n",
      "480001 training examples processsed\n",
      "481001 training examples processsed\n",
      "482001 training examples processsed\n",
      "483001 training examples processsed\n",
      "484001 training examples processsed\n",
      "485001 training examples processsed\n",
      "486001 training examples processsed\n",
      "487001 training examples processsed\n",
      "488001 training examples processsed\n",
      "489001 training examples processsed\n",
      "490001 training examples processsed\n",
      "491001 training examples processsed\n",
      "492001 training examples processsed\n",
      "493001 training examples processsed\n",
      "494001 training examples processsed\n",
      "495001 training examples processsed\n",
      "496001 training examples processsed\n",
      "497001 training examples processsed\n",
      "498001 training examples processsed\n",
      "499001 training examples processsed\n",
      "500001 training examples processsed\n",
      "501001 training examples processsed\n",
      "502001 training examples processsed\n",
      "503001 training examples processsed\n",
      "504001 training examples processsed\n",
      "505001 training examples processsed\n",
      "506001 training examples processsed\n",
      "507001 training examples processsed\n",
      "508001 training examples processsed\n",
      "509001 training examples processsed\n",
      "510001 training examples processsed\n",
      "511001 training examples processsed\n",
      "512001 training examples processsed\n",
      "513001 training examples processsed\n",
      "514001 training examples processsed\n",
      "515001 training examples processsed\n",
      "516001 training examples processsed\n",
      "517001 training examples processsed\n",
      "518001 training examples processsed\n",
      "519001 training examples processsed\n",
      "520001 training examples processsed\n",
      "521001 training examples processsed\n",
      "522001 training examples processsed\n",
      "523001 training examples processsed\n",
      "524001 training examples processsed\n",
      "525001 training examples processsed\n",
      "526001 training examples processsed\n",
      "527001 training examples processsed\n",
      "528001 training examples processsed\n",
      "529001 training examples processsed\n",
      "530001 training examples processsed\n",
      "531001 training examples processsed\n",
      "532001 training examples processsed\n",
      "533001 training examples processsed\n",
      "534001 training examples processsed\n",
      "535001 training examples processsed\n",
      "536001 training examples processsed\n",
      "537001 training examples processsed\n",
      "538001 training examples processsed\n",
      "539001 training examples processsed\n",
      "540001 training examples processsed\n",
      "541001 training examples processsed\n",
      "542001 training examples processsed\n",
      "543001 training examples processsed\n",
      "544001 training examples processsed\n",
      "545001 training examples processsed\n",
      "546001 training examples processsed\n",
      "547001 training examples processsed\n",
      "548001 training examples processsed\n",
      "549001 training examples processsed\n",
      "550001 training examples processsed\n",
      "551001 training examples processsed\n",
      "552001 training examples processsed\n",
      "553001 training examples processsed\n",
      "554001 training examples processsed\n",
      "555001 training examples processsed\n",
      "556001 training examples processsed\n",
      "557001 training examples processsed\n",
      "558001 training examples processsed\n",
      "559001 training examples processsed\n",
      "560001 training examples processsed\n",
      "561001 training examples processsed\n",
      "562001 training examples processsed\n",
      "563001 training examples processsed\n",
      "564001 training examples processsed\n",
      "565001 training examples processsed\n",
      "566001 training examples processsed\n",
      "567001 training examples processsed\n",
      "568001 training examples processsed\n",
      "569001 training examples processsed\n",
      "570001 training examples processsed\n",
      "571001 training examples processsed\n",
      "572001 training examples processsed\n",
      "573001 training examples processsed\n",
      "574001 training examples processsed\n",
      "575001 training examples processsed\n",
      "576001 training examples processsed\n",
      "577001 training examples processsed\n",
      "578001 training examples processsed\n",
      "579001 training examples processsed\n",
      "580001 training examples processsed\n",
      "581001 training examples processsed\n",
      "582001 training examples processsed\n",
      "583001 training examples processsed\n",
      "584001 training examples processsed\n",
      "585001 training examples processsed\n",
      "586001 training examples processsed\n",
      "587001 training examples processsed\n",
      "588001 training examples processsed\n",
      "589001 training examples processsed\n",
      "590001 training examples processsed\n",
      "591001 training examples processsed\n",
      "592001 training examples processsed\n",
      "593001 training examples processsed\n",
      "594001 training examples processsed\n",
      "595001 training examples processsed\n",
      "596001 training examples processsed\n",
      "597001 training examples processsed\n",
      "598001 training examples processsed\n",
      "599001 training examples processsed\n",
      "600001 training examples processsed\n",
      "601001 training examples processsed\n",
      "602001 training examples processsed\n",
      "603001 training examples processsed\n",
      "604001 training examples processsed\n",
      "605001 training examples processsed\n",
      "606001 training examples processsed\n",
      "607001 training examples processsed\n",
      "608001 training examples processsed\n",
      "609001 training examples processsed\n",
      "610001 training examples processsed\n",
      "611001 training examples processsed\n",
      "612001 training examples processsed\n",
      "613001 training examples processsed\n",
      "614001 training examples processsed\n",
      "615001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)\n",
    "\n",
    "## the following shows how to construct a graph with igraph\n",
    "## even though in this baseline we don't use it\n",
    "## look at http://igraph.org/python/doc/igraph.Graph-class.html for feature ideas\n",
    "\n",
    "#edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "\n",
    "## some nodes may not be connected to any other node\n",
    "## hence the need to create the nodes of the graph from node_info.csv,\n",
    "## not just from the edge list\n",
    "\n",
    "#nodes = IDs\n",
    "\n",
    "## create empty directed graph\n",
    "#g = igraph.Graph(directed=True)\n",
    " \n",
    "## add vertices\n",
    "#g.add_vertices(nodes)\n",
    " \n",
    "## add edges\n",
    "#g.add_edges(edges)\n",
    "\n",
    "# for each training example we need to compute features\n",
    "# in this baseline we will train the model on only 5% of the training set\n",
    "\n",
    "# randomly select 0.5% of training set\n",
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.005)))\n",
    "#training_set_reduced = [training_set[i] for i in to_keep]\n",
    "training_set_reduced = training_set\n",
    "\n",
    "# we will use three basic features:\n",
    "\n",
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "# abstarcts similarity \n",
    "abst_sim = []\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    \n",
    "    vec_source = vec_mean[index_source]\n",
    "    vec_target = vec_mean[index_target]\n",
    "    abst_sim.append(cosine_similarity(vec_source, vec_target))\n",
    "    \n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print (counter, \"training examples processsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "training_features = np.array([overlap_title, temp_diff, comm_auth, abst_sim]).T\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)\n",
    "\n",
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 testing examples processsed\n",
      "1001 testing examples processsed\n",
      "2001 testing examples processsed\n",
      "3001 testing examples processsed\n",
      "4001 testing examples processsed\n",
      "5001 testing examples processsed\n",
      "6001 testing examples processsed\n",
      "7001 testing examples processsed\n",
      "8001 testing examples processsed\n",
      "9001 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "11001 testing examples processsed\n",
      "12001 testing examples processsed\n",
      "13001 testing examples processsed\n",
      "14001 testing examples processsed\n",
      "15001 testing examples processsed\n",
      "16001 testing examples processsed\n",
      "17001 testing examples processsed\n",
      "18001 testing examples processsed\n",
      "19001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "21001 testing examples processsed\n",
      "22001 testing examples processsed\n",
      "23001 testing examples processsed\n",
      "24001 testing examples processsed\n",
      "25001 testing examples processsed\n",
      "26001 testing examples processsed\n",
      "27001 testing examples processsed\n",
      "28001 testing examples processsed\n",
      "29001 testing examples processsed\n",
      "30001 testing examples processsed\n",
      "31001 testing examples processsed\n",
      "32001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# we need to compute the features for the testing set\n",
    "\n",
    "overlap_title_test = []\n",
    "temp_diff_test = []\n",
    "comm_auth_test = []\n",
    "abst_sim_test = []\n",
    "\n",
    "   \n",
    "counter = 0\n",
    "for i in range(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    vec_source = vec_mean[index_source]\n",
    "    vec_target = vec_mean[index_target]\n",
    "    abst_sim_test.append(cosine_similarity(vec_source, vec_target))\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print (counter, \"testing examples processsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "\n",
    "#testing_features = np.array([np.array(overlap_title_test),np.array(temp_diff_test),np.array(comm_auth_test), np.array(abst_sim_test)]).T\n",
    "testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test, abst_sim_test]).T\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)\n",
    "\n",
    "predictions_SVM = list(classifier.predict(testing_features))\n",
    "\n",
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "predictions_SVM = zip(range(len(testing_set)), predictions_SVM)\n",
    "\n",
    "with open(\"improved_predictions.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    for row in predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32648,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(overlap_title_test,dtype=float).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32648,), (32648,), (32648,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(temp_diff_test).shape,np.array(comm_auth_test).shape,np.array(abst_sim_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abst_sim_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
